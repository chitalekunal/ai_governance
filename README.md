# Bias-Aware Credit Approval System (EU AI Act Compliance)
Normally, an AI is like a sponge‚Äîit soaks up everything in the data you give it. If your historical data shows that banks mostly gave loans to men in the 90s, the AI thinks, _"Ah, being a man must be why they are good at paying back loans!"_ ü§¶‚Äç‚ôÇÔ∏è This is a __bias__.

A "Bias-Aware" system is one where you, the developer, have added a __special filter__ to make sure the AI isn't using illegal shortcuts to make decisions.
## 1. The "What & Why"
I‚Äôve been hearing a lot about how AI is getting regulated in Europe (the EU AI Act), so I wanted to see if I could actually "break" a credit-scoring model and then fix it.
The goal here isn't just to make a model that's 99% accurate. It‚Äôs to build a model that doesn't accidentally become a jerk by discriminating against people based on their gender or age. In the EU, if your AI does that, you're in big trouble‚Äîso I‚Äôm playing "AI Ethics Lead" for a day to see how it works. üòÅ

## 2. How I know it discriminates?
In Europe, the concern about AI bias isn't just a "gut feeling"‚Äîit's based on high-profile cases where algorithms actually failed, combined with a very specific legal philosophy about human rights. notifications of The Dutch "Childcare Benefit" Scandal , The "CV Ghosting" in Recruitment, Predictive Policing in Belgium & Italy are some of them. Well I am not targetting to solve all of them but surely this usecase can help understand how it all works and how companies can buidl a robust system around it. I have compiled some examples along with the news article from internet. Not necessarily these are related to AI. As data generated by such situations is also logged and could be used by the system. This becomes important to note such stories.

### 2.1 The Dutch "Childcare Benefit" Scandal (Toeslagenaffaire)
* __What happened__: The Dutch tax authorities used a self-learning algorithm to create "risk profiles" for families claiming childcare benefits to spot fraud.<br/>
* __The Bias__: The algorithm penalized people based on dual nationality and low income.<br/>
* __The Result__: 26,000 parents were wrongly accused of fraud, forced to pay back tens of thousands of Euros, leading to bankruptcies and broken homes.<br/>
* __Governance Lesson__: In 2022, the Dutch government was fined ‚Ç¨3.7 million for GDPR violations, proving that "the algorithm made me do it" is not a legal defense.<br/>
* __Article__: [Dutch childcare benefit scandal an urgent wake-up call to ban racist algorithms](https://www.amnesty.org/en/latest/news/2021/10/xenophobic-machines-dutch-child-benefit-scandal/)
### 2.2. The "CV Ghosting" in Recruitment (Multiple EU Firms)
* __What happened__: AI tools trained on the last 10 years of successful resumes learned that "successful senior engineers" were almost exclusively male.<br/>
* __The Bias__: The AI began downgrading any resume that mentioned "Women's" (e.g., "Women‚Äôs University" or "Captain of Women‚Äôs Basketball"), even if the candidate was more qualified.<br/>
* __Governance Lesson__: Under the EU AI Act, recruitment AI is now strictly "High-Risk," requiring constant bias audits .<br/>
* __Article__: [Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G/)
### 2.3. Predictive Policing in Belgium & Italy
* __What happened__: Police departments tested AI to predict where crimes would happen.<br/>
* __The Bias__: Because the AI was fed historical arrest data, it kept sending police back to the same marginalized neighborhoods, creating a "feedback loop." More police in one area lead to more arrests, which the AI then used to justify even more police.<br/>
* __Governance Lesson__: Individual "predictive policing" (profiling individuals) is now largely prohibited or strictly limited in the EU because it violates the "Presumption of Innocence."<br/>
* __Article__ : [Belgium: New report calls for a ban on 'predictive' policing technologies](https://www.statewatch.org/news/2025/april/belgium-new-report-calls-for-a-ban-on-predictive-policing-technologies/)
### 2.4. The "Post Office" System Error (UK/Europe Context)
* __What happened__: A faulty accounting system (Horizon) made it look like shop managers were stealing money. Instead of questioning the computer, the company prosecuted hundreds of innocent people.
* __Governance Lesson__: It taught Europe that Human Oversight (having a person who can override the computer) is the most important part of any system.
* __Article__: [Post Office scandal may have led to more than 13 suicides, inquiry finds](https://www.theguardian.com/uk-news/2025/jul/08/post-office-scandal-inquiry-horizon-it-scandal)
