# Bias-Aware Credit Approval System (EU AI Act Compliance)
Normally, an AI is like a sponge‚Äîit soaks up everything in the data you give it. Most simple example that I can think of is when a lending institution uses historical data that shows institution mostly gave loans to men in the 90s, the AI thinks, _"Ah, being a man must be why they are good at paying back loans!"_ ü§¶‚Äç‚ôÇÔ∏è This is a __bias__.

A "Bias-Aware" system is one where you, the developer, have added a __special filter__ to make sure the AI isn't using illegal shortcuts to make decisions.
## 1. The "What & Why"
I‚Äôve been hearing a lot about how AI is getting regulated in Europe (the EU AI Act), so I wanted to see if I could actually "break" a credit-scoring model and then fix it.
The goal here isn't just to make a model that's 99% accurate. It‚Äôs to build a model that doesn't accidentally become a jerk by discriminating against people based on their gender or age. In the EU, if your AI does that, you're in big trouble‚Äîso I‚Äôm playing "AI Ethics Lead" for a day to see how it works. üòÅ

## 2. How I know it discriminates?
In Europe, the concern about AI bias isn't just a "gut feeling"‚Äîit's based on high-profile cases where algorithms actually failed, combined with a very specific legal philosophy about human rights. notifications of The Dutch "Childcare Benefit" Scandal , The "CV Ghosting" in Recruitment, Predictive Policing in Belgium & Italy are some of them. Well I am not targetting to solve all of them but surely this usecase can help understand how it all works and how companies can buidl a robust system around it. I have compiled some examples along with the news article from internet. Not necessarily these are related to AI. As data generated by such situations is also logged and could be used by the system. This becomes important to note such stories.

### 2.1 The Dutch "Childcare Benefit" Scandal (Toeslagenaffaire)
* __What happened__: The Dutch tax authorities used a self-learning algorithm to create "risk profiles" for families claiming childcare benefits to spot fraud.<br/>
* __The Bias__: The algorithm penalized people based on dual nationality and low income.<br/>
* __The Result__: 26,000 parents were wrongly accused of fraud, forced to pay back tens of thousands of Euros, leading to bankruptcies and broken homes.<br/>
* __Governance Lesson__: In 2022, the Dutch government was fined ‚Ç¨3.7 million for GDPR violations, proving that "the algorithm made me do it" is not a legal defense.<br/>
* __Article__: [Dutch childcare benefit scandal an urgent wake-up call to ban racist algorithms](https://www.amnesty.org/en/latest/news/2021/10/xenophobic-machines-dutch-child-benefit-scandal/)
### 2.2. The "CV Ghosting" in Recruitment (Multiple EU Firms)
* __What happened__: AI tools trained on the last 10 years of successful resumes learned that "successful senior engineers" were almost exclusively male.<br/>
* __The Bias__: The AI began downgrading any resume that mentioned "Women's" (e.g., "Women‚Äôs University" or "Captain of Women‚Äôs Basketball"), even if the candidate was more qualified.<br/>
* __Governance Lesson__: Under the EU AI Act, recruitment AI is now strictly "High-Risk," requiring constant bias audits .<br/>
* __Article__: [Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G/)
### 2.3. Predictive Policing in Belgium & Italy
* __What happened__: Police departments tested AI to predict where crimes would happen.<br/>
* __The Bias__: Because the AI was fed historical arrest data, it kept sending police back to the same marginalized neighborhoods, creating a "feedback loop." More police in one area lead to more arrests, which the AI then used to justify even more police.<br/>
* __Governance Lesson__: Individual "predictive policing" (profiling individuals) is now largely prohibited or strictly limited in the EU because it violates the "Presumption of Innocence."<br/>
* __Article__ : [Belgium: New report calls for a ban on 'predictive' policing technologies](https://www.statewatch.org/news/2025/april/belgium-new-report-calls-for-a-ban-on-predictive-policing-technologies/)
### 2.4. The "Post Office" System Error (UK/Europe Context)
* __What happened__: A faulty accounting system (Horizon) made it look like shop managers were stealing money. Instead of questioning the computer, the company prosecuted hundreds of innocent people.
* __Governance Lesson__: It taught Europe that Human Oversight (having a person who can override the computer) is the most important part of any system.
* __Article__: [Post Office scandal may have led to more than 13 suicides, inquiry finds](https://www.theguardian.com/uk-news/2025/jul/08/post-office-scandal-inquiry-horizon-it-scandal)

## 3. What causes these problems
An AI algorithm is nothing but an advanced pattern recognition algorithm. AI doesn't "think"; it finds patterns in historical data.
### 3.1. The "Garbage In, Garbage Out" Reality üóëÔ∏è ‚û°Ô∏è üìâ
AI doesn't "think"; it finds patterns in historical data. For example, if a bank's historical data shows they mostly gave loans to men in the 1990s, then AI learns that "being a man" is a feature of a good borrower. It starts rejecting women because it was feed in old prejudices.
### 3.2. The "Black Box" Problem üì¶ ‚ùì
Many advanced AI systems are so complex that even the people who built them can't explain exactly why a specific person was rejected for a job or a loan. In the EU, you have a "Right to Explanation" under GDPR. If an AI rejects your apartment application and the company says, "We don't know why, the computer just said no," that is considered a violation of your rights.
### 3.3. Famous Local "Fails" üö®
An algorithm used to spot fraud wrongly flagged thousands of families‚Äîmostly ethnic minorities‚Äîforcing them into debt and poverty. It was a massive wake-up call that "neutral" math can have devastating human consequences.
### 3.4. Protecting "Fundamental Rights" ‚öñÔ∏è
Unlike some other regions that prioritize innovation first, Europe prioritizes human dignity first. The EU AI Act classifies things like credit scoring, hiring, and policing as "High-Risk." They believe that if we don't force AI to be fair now, we will accidentally build a "digital caste system" where your race, gender, or even your zip code determines your future without you ever knowing why.

## These are just one offs, AI can't do like that, can it ? Lets check
AI being nothing more than an advanced pattern matching algorithm, its inference is based on probablity. If the data used to train the model is biased this biasness is passed down to the model training it to be bais towards certain category of people. here is my "Game Plan" to affirm it.
#### Step 1: The "Lazy" Model
First, I‚Äôm just going to train a standard model (Random Forest) and see what happens. Usually, these models "cheat" by picking up on biases in the data. If the model is approving way more men than women just because of historical data, we've failed.
#### Step 2: Peeking Inside (Explainability)
I'm using SHAP to see what the AI is actually thinking. Is it looking at "Credit History," or is it secretly obsessing over "Personal Status"? If it's the latter, we have a "Digital Redlining" problem.
#### Step 3: The "Fix" (Mitigation)
This is the fun part. I‚Äôm using a technique called Exponentiated Gradient. Basically, I‚Äôm telling the AI: _"You can be as smart as you want, but you __MUST__ keep the approval rates fair between groups."_ 
#### ‚öñÔ∏è Step 4: The Reality Check
Finally, I‚Äôll compare the "Fair" model to the "Fast" model. _Spoiler alert: making a model ethical usually makes it slightly less accurate. In this project, I'm deciding if that "fairness tax" is worth it (Hint: in Europe, it always is)_.
